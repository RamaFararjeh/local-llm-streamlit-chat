# ğŸ¤– Local LLM Streamlit Chat (Ollama)

A simple and interactive **Streamlit-based chat interface** connected to a **locally hosted Large Language Model (LLM)** using **Ollama**.  
The application runs fully **offline**, supports **conversation history**, **multi-day chat storage**, and provides a clean chat UI similar to modern LLM applications.

---

## ğŸš€ Features

- ğŸ§  Connects to a **locally installed LLM** via Ollama  
- ğŸ’¬ Real-time chat interface built with **Streamlit**
- ğŸ—‚ï¸ **Conversation history** with full context awareness
- ğŸ“… **Multi-day chat storage** (one JSON file per day)
- ğŸ”„ Reset and clear chat functionality
- âš™ï¸ Model selection from the sidebar
- ğŸŒ Fully **local execution** (no external APIs required)

---

## ğŸ› ï¸ Tech Stack

- **Python**
- **Streamlit** â€“ Frontend UI
- **Ollama** â€“ Local LLM runtime
- **Requests** â€“ API communication
- **JSON** â€“ Persistent chat storage

---

## ğŸ“‚ Project Structure

local-llm-streamlit-chat/
â”‚
â”œâ”€â”€ app.py # Main Streamlit application
â”œâ”€â”€ README.md # Project documentation
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ chats/
â””â”€â”€ .gitkeep # Chat history JSON files (created at runtime)


---

## âš™ï¸ Prerequisites

Before running the project, make sure you have:

- Python **3.9+**
- Ollama installed and running  
  ğŸ‘‰ https://ollama.com
- At least one local model pulled, for example:
  ```bash
  ollama pull llama3.2
